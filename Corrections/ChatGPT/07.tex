% Options for packages loaded elsewhere

%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{}
\date{}

\begin{document}

{Corrections and Elaborations on Key Concepts from the
Seminar}\label{corrections-and-elaborations-on-key-concepts-from-the-seminar}

\begin{itemize}
\item
  \textbf{Strengths of PINNs:}

  \begin{itemize}
  
  \item
    \textbf{Incorporating Physical Laws:} PINNs integrate physical laws
    and constraints directly into the learning process. They do this by
    incorporating the residual of the governing partial differential
    equations (PDEs) into the loss function during training. This
    ensures that the trained neural network respects the underlying
    physics of the system.
  \item
    \textbf{Improved Accuracy:} The incorporation of physical laws can
    enhance the accuracy of the model, especially in scenarios where
    data is scarce or noisy. By constraining the solution space using
    known physical principles, PINNs can extrapolate and generalize
    better than purely data-driven models.
  \item
    \textbf{Reduced Data Requirements:} PINNs often require less
    training data compared to traditional machine learning models. This
    is a significant advantage in scientific applications where
    obtaining large, high-quality datasets can be challenging and
    expensive.
  \end{itemize}

  \textbf{Limitations of PINNs:}

  \begin{itemize}
  
  \item
    \textbf{High Computational Cost:} Training PINNs can be
    computationally expensive, especially for complex systems and
    high-dimensional problems. The inclusion of PDE residuals in the
    loss function requires evaluating derivatives using techniques like
    automatic differentiation, which can add computational overhead.
  \item
    \textbf{Challenging Training:} Training PINNs can be difficult,
    especially for hyperbolic PDEs. Hyperbolic PDEs, which describe
    phenomena like wave propagation and fluid flow, often exhibit
    complex behavior that can be challenging for neural networks to
    learn.
  \item
    \textbf{Difficulty Handling Complex Physical Systems:} PINNs may
    struggle to accurately model systems with highly complex physics,
    such as those involving turbulence or multi-scale phenomena. These
    limitations arise from the inherent challenges of representing and
    approximating complex physical processes using neural networks.
  \end{itemize}
\item
  \textbf{Reason for Transition to ROMs:} The research team transitioned
  from PINNs to ROMs to address the limitations of PINNs, primarily
  focusing on improving computational efficiency and enabling the
  handling of larger-scale simulations. ROMs are particularly
  well-suited for multi-query applications, such as optimization,
  inverse problems, and uncertainty quantification, where computational
  cost is a significant factor.
\item
  \textbf{ROMs Acceleration and Trade-offs:}

  \begin{itemize}
  
  \item
    \textbf{Acceleration Mechanism:} ROMs speed up simulations by
    constructing a reduced-order representation of the system's
    dynamics. This is achieved by identifying a low-dimensional basis
    that captures the dominant features of the system's behavior. By
    projecting the original high-dimensional system onto this
    low-dimensional basis, ROMs can significantly reduce the
    computational complexity of simulations.
  \item
    \textbf{Trade-offs:} While ROMs offer substantial speed advantages,
    they often come at the cost of reduced accuracy, especially for
    complex or nonlinear systems. By focusing on the dominant features
    and ignoring smaller-scale details, ROMs introduce a degree of
    approximation. The trade-off between accuracy and computational
    efficiency is a key consideration when applying ROMs.
  \end{itemize}
\item
  \textbf{Three ML-based Models and Operator Inference:}

  \begin{itemize}
  
  \item
    \textbf{Prominent Models:} The seminar highlights DeepONets,
    equivariant neural networks, and operator inference as promising
    models for scientific machine learning.

    \begin{itemize}
    
    \item
      DeepONets are based on the universal approximation theorem for
      operators, providing a strong theoretical foundation for their
      convergence.
    \item
      Equivariant neural networks are designed to preserve specific
      symmetries inherent in the physical laws governing a system,
      ensuring that the learned model respects these symmetries.
    \item
      Operator inference focuses on learning the operators that govern
      the system's dynamics, enabling better generalization and
      extrapolation capabilities.
    \end{itemize}
  \item
    \textbf{Operator Inference Advantages:} Among these models, operator
    inference emerges as a preferred choice due to its computational
    efficiency and reliance on simple regression methods with
    straightforward regularization. Its ability to learn the system's
    governing operators provides a more physically interpretable
    representation, enhancing understanding and predictive capabilities.
  \end{itemize}
\item
  \textbf{Roadmap for Quantum Computing in Greenhouse Gas Emission
  Estimation:} The roadmap for integrating quantum computing into
  greenhouse gas emission estimation involves a multi-step approach:

  \begin{itemize}
  
  \item
    \textbf{Data Synthesis:} Using classical simulators to generate
    realistic data for methane plume dispersion.
  \item
    \textbf{Operator Inference Development:} Building a classical
    operator inference model as a foundation.
  \item
    \textbf{Quantum Algorithm Integration:} Replacing the classical
    regression component of the operator inference model with a quantum
    regression algorithm.
  \item
    \textbf{Hybrid Model Validation:} Testing and validating the hybrid
    classical-quantum operator inference model using both simulated and
    real-world data.
  \item
    \textbf{Deployment and Utility Demonstration:} Deploying the
    quantum-enhanced model for improved greenhouse gas emission
    estimation and showcasing the utility of quantum computing in a
    real-world application.
  \end{itemize}

  \textbf{Key Emphasis:} The roadmap leverages a hybrid
  classical-quantum approach, taking advantage of the strengths of both
  classical and quantum computing. It aims to enhance the accuracy and
  efficiency of emission estimations by using quantum algorithms for
  tasks that are particularly well-suited for quantum computers, such as
  regression.
\end{itemize}

\end{document}
